{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/RadionBik/Statistical-estimator-of-network-traffic\n",
    "\n",
    "This is notebook is under active development. DO NOT expect something meaningful here.\n",
    "\n",
    "The idea is to utilize RNN as a model for predicting network traffic time-series (e.g. packet length and IAT)\n",
    "\n",
    "Currently, many-to-many architecture is being tested.\n",
    "\n",
    "Many-to-one is the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and normalize original traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"..\")\n",
    "from stat_estimator import *\n",
    "from packet_transceiver import *\n",
    "from helper_functions import *\n",
    "from hmm_helpers import *\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "pcapfile = 'traffic_dumps/skypeLANhome.pcap'\n",
    "traffic_dfs = getTrafficFeatures(pcapfile,'all','flow',(1,99),100)[0]\n",
    "norm_traffic, scalers = normalize_dfs(traffic_dfs, std_scaler=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and test data for RNN\n",
    "\n",
    "2 approaches:\n",
    "\n",
    "1. Windowed slicing of time-series with step=1 to allow for more testing data.\n",
    "2. Batched slicing of data, when we just split all the data into chunks without overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_training_set(df, window_size, shift=1):\n",
    "    feature_number = df.shape[1]\n",
    "    sample_number = df.shape[0]\n",
    "    X = np.zeros((sample_number-window_size+1, window_size, feature_number))\n",
    "    y = np.zeros((sample_number-window_size+1, window_size, feature_number))\n",
    "\n",
    "    for batch in range(sample_number-window_size+1):\n",
    "        X[batch, :, :] = df.iloc[batch:batch+window_size, :]\n",
    "        y[batch, :, :] = df.shift(shift).fillna(0).iloc[batch:batch+window_size, :]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_batched_training_set(df, window_size, shift=1):\n",
    "    batch_num = int(np.ceil(df.shape[0]/window_size))\n",
    "    zeros_to_append = np.zeros((batch_num*window_size - df.shape[0], df.shape[1]))\n",
    "    df = df.append(pd.DataFrame(zeros_to_append, columns=df.columns), ignore_index=True)\n",
    "\n",
    "    X = np.reshape(df.values, (batch_num, window_size, 2)) \n",
    "    y = np.reshape(df.shift(shift).fillna(0).values, (batch_num, window_size, 2) )\n",
    "    \n",
    "    #return only non-zero batches \n",
    "    return X[1:,:,:], y[1:,:,:]\n",
    "\n",
    "for df in iterate_dfs(norm_traffic):\n",
    "    #df.plot(subplots=True, layout = (2,1))\n",
    "    y = df.shift(1).fillna(0).values\n",
    "\n",
    "#prepare data\n",
    "\n",
    "window_size = 100\n",
    "\n",
    "feature_number = df.shape[1]\n",
    "sample_number = df.shape[0]\n",
    "\n",
    "\n",
    "X, y = get_windowed_training_set(df, window_size)\n",
    "#X, y = get_batched_training_set(df, window_size)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "    \n",
    "#https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/ \n",
    "\n",
    "#https://stackoverflow.com/questions/38714959/understanding-keras-lstms?rq=1\n",
    "\n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "#help(pd.DataFrame.append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, GRU\n",
    "\n",
    "def get_random_model_parameters(input_size):\n",
    "    \n",
    "    return {'numb_of_layers' : np.random.choice([1,2,3,4]),\n",
    "          'layer_nodes' : [input_size] + [int(np.random.choice([0.5*input_size, input_size, 2*input_size])) for i in range(3)],\n",
    "          'dropouts' : [np.random.choice([0.05,0.1,0.2,0.3]) for i in range(4)] }\n",
    "\n",
    "def set_model_parameters(numb_of_layers, layer_nodes, dropouts):\n",
    "    return {'numb_of_layers' : numb_of_layers,\n",
    "            'layer_nodes' : layer_nodes,\n",
    "            'dropouts' : dropouts }\n",
    "\n",
    "def build_gru_model(model_parameters, input_size, feature_number):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(GRU(units=model_parameters['layer_nodes'][0], \n",
    "                  input_shape=(input_size,  feature_number), \n",
    "                  return_sequences=True, \n",
    "                  dropout=model_parameters['dropouts'][0]))\n",
    "\n",
    "    for layer in range(1,model_parameters['numb_of_layers']):\n",
    "        model.add(GRU(units=model_parameters['layer_nodes'][layer], \n",
    "                      return_sequences=True,\n",
    "                      dropout=model_parameters['dropouts'][layer]))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(feature_number, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_lstm_model(model_parameters, input_size, feature_number):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(LSTM(units=model_parameters['layer_nodes'][0], \n",
    "                  input_shape=(input_size,  feature_number), \n",
    "                  return_sequences=True, \n",
    "                  dropout=model_parameters['dropouts'][0]))\n",
    "\n",
    "    for layer in range(1,model_parameters['numb_of_layers']):\n",
    "        model.add(LSTM(units=model_parameters['layer_nodes'][layer], \n",
    "                      return_sequences=True,\n",
    "                      dropout=model_parameters['dropouts'][layer]))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(feature_number, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "#for i in range(8):\n",
    "\n",
    "params = get_random_model_parameters(window_size)\n",
    "\n",
    "#model = build_gru_model(params, window_size, feature_number)\n",
    "model = build_lstm_model(params, window_size, feature_number)\n",
    "\n",
    "\n",
    "# define the checkpoint\n",
    "checkpoint_path=\"rnn_models/lstm-loss-{loss:.4f}-epochs-{epoch:02d}\"+\\\n",
    "                \"-{}layers-{}x{}x{}-drops-{}x{}x{}-batched.hdf5\".format(params['numb_of_layers'],\n",
    "                                                              *params['layer_nodes'],\n",
    "                                                              *params['dropouts'])\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 #save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 #save_best_only=True,\n",
    "                                                 monitor='loss',\n",
    "                                                 period = 20)\n",
    "\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=10, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model('rnn_models/gru-loss-0.4782-epochs-200-2layers-100x50x50-drops-50x0.05x0.05-batched.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"rnn_models/lstm-loss-0.4560-epochs-10-{}layers-{}x{}x{}-drops-{}x{}x{}-batched.hdf5\".format(params['numb_of_layers'],\n",
    "                                                              *params['layer_nodes'],\n",
    "                                                              *params['dropouts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number_to_gener = 6000\n",
    "\n",
    "def generate_batched_samples(model, init_X, sample_number_to_gener, window_size, scalers, feature_number=2):\n",
    "\n",
    "    #when shift=window size\n",
    "    batch_number = int(np.ceil(sample_number_to_gener/window_size))\n",
    "    predicted_X = np.zeros((batch_number, window_size, feature_number))\n",
    "\n",
    "    for device, direction, scaler in iterate_traffic_dict(scalers):\n",
    "        pass\n",
    "\n",
    "    X_to_predict = init_X\n",
    "    for batch in range(batch_number):\n",
    "        \n",
    "        predicted_X[batch, :, :] = model.predict(X_to_predict)\n",
    "        X_to_predict = predicted_X[batch:batch+1:, :, :]\n",
    "\n",
    "    return pd.DataFrame(scaler.inverse_transform( predicted_X.reshape(-1, predicted_X.shape[-1]) ))\n",
    "\n",
    "def generate_windowed_samples(model, init_X, sample_number_to_gener, window_size, scalers, shift=1, feature_number=2):\n",
    "    \n",
    "    gen_X = np.zeros((sample_number_to_gener, feature_number))\n",
    "    \n",
    "    for device, direction, scaler in iterate_traffic_dict(scalers):\n",
    "        pass\n",
    "\n",
    "    X_to_predict = init_X\n",
    "    batch_number = int(np.ceil(sample_number_to_gener/window_size))\n",
    "    for batch in range(batch_number):\n",
    "        X_to_predict = model.predict(X_to_predict)\n",
    "        gen_X[batch*window_size:(batch+1)*window_size, :] = X_to_predict\n",
    "    \n",
    "    return pd.DataFrame(scaler.inverse_transform(gen_X))\n",
    "\n",
    "gener_df = generate_windowed_samples(model, X[100:101:,:,:], sample_number_to_gener, window_size, scalers)\n",
    "gener_df.hist(bins=30)\n",
    "generate_batched_samples(model, X[0:1:,:,:], sample_number_to_gener, window_size, scalers).hist(bins=30)\n",
    "#gener_df.hist(bins=30)\n",
    "pd.DataFrame(scaler.inverse_transform(df)).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gener_df.index = pd.to_datetime(gener_df.iloc[:,0].cumsum(), unit='s')\n",
    "goodput_dfs = gener_df.resample('1S').sum()\n",
    "#plt.figure()\n",
    "ax = (goodput_dfs[1]/1024).plot(grid=True, label=direction, lw=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#background process\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def spatial_correlation_factor(distance, alpha):\n",
    "    return np.exp(-alpha * distance)\n",
    "\n",
    "spatial_correlation_factor(30, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
